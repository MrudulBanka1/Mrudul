# Mrudul Banka's Data Science Portfolio

Hello, I am Mrudul Banka. I like to explore, learn, work and innovate. 

My areas of interest are - data science, operations research, IoT analytics, robotics, economics and process improvement. 

This page showcases my brilliant experience during my internship and data science projects I have worked on till date. 

## Internship experience at Apilation.ai

- __Company Website:__ [Apilation.ai](http://www.apilation.ai/)   
- __Location: Mill Valley, California - Dallas, Texas - Remote__
- __Position: Big Data Analytics Intern__  
- __Duration: August 2018 - May 2019__

__About Apilation.ai__

   Apilation.ai is an intelligent information mining company designed from the ground up to innovate, develop, and deliver on 
the Big Data analytics needs of our post-IOT data-centric society by providing the people, process, and technologies that 
empower our clients to: 1) Connect ubiquitously, 2) Analyze instinctively, and 3) Act intelligently. All in real-time.

__My Experience:__

1. Overall it was a wonderful learning experience which has given me a great outlook of the data
science industry i.e.
   - __to achieve success as a data science team, the team has to develop a thorough understanding of the business problem along with        technical expertise.__ 
   - __The team has to work towards developing trust of decision makers, dealing patiently with all stakeholders.__ 
   - __Another important aspect is how concisely can the team convey the impact which can achieved by pursuing a project when connecting     with the right people.__ 
2. Working on industrial scale Big Data projects showed me the how information generated
through big data analytics and machine learning can be used to generate and save millions of
dollars in revenue.
3. I was provided an opportunity to lead a project where I was the point of contact with client. This
opportunity helped me grow as a professional. I was also asked to train my fellow interns on a
technology I was good with.
4. Throughout the internship I was mentored thoroughly by the CEO which proved helpful to me
on a number of occasions.
5. Learned and worked on new programming languages, worked the NoSQL databases with JSonar Studio and MongoDB, 
how to create visualization on edge technologies tool. I was able to explore and experiment with all the tools 
within my sandbox environment which helped me learn faster.
6. This internship prepared me for intense meetings, how to communicate with clients and how to sell
your product. My overall communication skills are far superior than what they were before I
started the internship.
7. I understood how effective working in an open agile environment is with the key knowledge of how to tackle
client requirements and produce deliverables on time. 

__My Responsibilities:__

   As described in the offer letter – ‘’Your job is to help us re-imagine how technology is
leveraged in the businesses of the future. You will work under the mentorship of an experienced
team leader in either Analytics, Platform, or Experience Engineering; learning new tools, techniques,
and skills that can include coding predictive algorithms, building software, and defining how users
experience our offerings, or all of the above. You can expect to have substantial influence on the
direction that your work, your team, and the company takes as we move forward. Examples of the
types of skills we expect for you to possess coming in include, but are not limited to the following:
Education in Data Science and Business Analytics and expertise in tools such as SQL, R, Python SAS,
etc. Java-Script Notation (JSON), MongoDB, etc. You should understand how queries, joins, and
aggregation pipelines work. Education in Web Design and expertise such as the MEAN stack
(MongoBD, Express, Angular, Node), HTML5, CSS, etc. SQL skills also important. Visualization tools
such as Tableau, Domo, Spotfire, and PowerBI.’’

   Along with everything quoted above, I was also made the team lead on DS-1 reconciliation engine as 
well as the point of contact for the project. 

__Projects Completed at internship at Apilation.ai__

1. To build an automated DS-1 reconciliation engine, analyze the discrepancies, fault detection
   by rules and its visualization of Tariff # 14 for Frontier Communications.
2. To develop, analyze and deploy a machine learning model to predict the chances of failure
   of set top boxes in the state of XXXX for Frontier Communications.
3. To build an automated DS-1 reconciliation engine, analyze the discrepancies, fault detection
   by rules and its visualization of Tariff # 2,3,5,10 for Frontier Communications.
4. To build a recommendation engine to recommend Tv shows a user would watch given his past 
   viewership and segments to which it belonged.
5. Perform Data Profiling to understand what can be done to obtain business value from their data.

I am unable to post links to my data science projects I had completed during my internship due to proprietary concerns but here is a description of all the work I have been involved in. 

__Project 1: Automated reconciliation engine – Phase 1__

   The project is to build an automated DS-1 reconciliation engine. Frontier Communications has
come together through a long series of acquisitions. It is believed that their billing is incorrect in a small
percentage of cases, but a small percentage of a lot of circuits can make up a lot of money for the
company. For the first phase, they had decided to concentrate just on the single tariff #14 which covers 43% of
the 170,000 circuits. Project was divided into two sets of rules on based on which we can classify the circuits as billed
correctly or billed incorrectly. After classification and implementation of production version of application,
Processed data, statistical analysis and visualizations were presented to the client. The application was built in
over 4 weeks in JSonar studio.

__My Contribution to project:__

1. Developed logic for exclusion rules and created pipelines in JSonar to execute those rules.
2. Created more than 40 pipelines for production environment of the application.
3. Designed statistical parameters through which outputs should be processed.
4. Created Visualizations for analysis. Performed analysis and hypothesis testing of incorrectly
billed circuits.
5. Conducted testing and performed debugging for business rules pipelines developed by other
members of the team.
6. Presented weekly status update on the project to the client.

__Project 2: Machine learning model for Set top Box__

   The project is to build a prediction model to detect which set up box is going to fail. Frontier
communications offer digital television services across USA. To minimize service order, minimize the
time taken to fulfill service orders because of set top box failures, frontier is turning to machine learning
to detect which set up box is about to fail based on the data gathered from the set top box every 15
minutes. For this project, we worked with dataset gathered from the state of XXXX. An approximate
total of 9TB of data was processed with a 90Gb batch of data inflow occurring every 15 mins. 6 different
data mining models were created for 6 KPI’s which would indicate a fail/no fail status prediction of set
top box. The application was built in over 4 weeks in JSonar studio, R Studio and H20 machine learning and python

__My Contribution to project:__

1. Performed feature engineering to select features, performed principle component analysis.
2. Used concepts of regression analysis to find correlation for time series data.
3. Used the H20 machine learning library along with R and python  to develop machine learning models for
failure prediction. Best model had a prediction accuracy of 91.01% (Model Type: xgboost).
4. Tested models developed by other engineers in the team.
5. Build production pipelines for a final equation with all 6 kpi's combining to create a score for prediction. 

__Project 3: Automated reconciliation engine – Phase 2 – Project Lead__

The project is to build an automated DS-1 reconciliation engine. Frontier Communications has
come together through a long series of acquisitions. It is believed that their billing is incorrect in a small
percentage of cases, but a small percentage of a lot of circuits can make up a lot of money for the
company. For the second phase, they had decided to concentrate on multiple tariffs #2, 3, 5, 10, which covers
32% of the 170,000 circuits. Project was divided into two sets of rules on based on which we can classify the circuits
as billed correctly or billed incorrectly. After classification and implementation of production version of
application, Processed data, statistical analysis and visualizations were presented to the client. The application
was built in over 4 weeks in JSonar studio.

__My Contribution to project:__

1. I was the project lead for phase 2 and point of contact for client.
2. Developed logic for business rules and created pipelines in JSonar to execute those rules.
3. Created more than 85 pipelines for production environment of the application.
4. Designed statistical parameters through which outputs should be processed.
5. Created Visualizations for analysis. Performed analysis and hypothesis testing of incorrectly
billed circuits.
6. Conducted testing and performed debugging for business rules pipelines developed by other
members of the team.
7. Presented weekly status update on the project to the client.

__Project 4: Tv shows recommendation engine__

In this project we were asked to build a recommendation engine to recommend Tv programs a customer would like based on set top box’s viewership history. This recommendation engine could be used a marketing tool for Set top users to recommend them shows. A channel churn recommendation engine was also built to understand if a user was likely to churn a particular channel user is currently subscribed to. We used IMDB data to understand what genres the Tv shows belonged to. The application was built in 4 weeks.    

__My Contribution to project:__
1. Building the recommendation engine for Tv shows and channel churn based on cosine similarity, collaborative filtering and Simon Funk'S SVD.
2. Performed feature engineering to extract data from Jsonar, clean and normalize the necessary fields. 
3. Create visualizations to take user id as input and recommend programs
4. Created Segments and used  IMDB dataset to enrich the data by adding genres to understand what is being watched.

__Project 5: Data Profiling for company XXXX__

We received a telecom company's mobile application log data. The expectation from us was to look into the data and to understand if any business value can be generated from that data. The data contained timestamp, location with other parameters relating to user which cannot be disclosed. My approach was to find answers for 2 questions - 1. How can I help you improve your product? 2. Can we build a new product from the information we posses? 

__My Contribution to project:__
1. Understand the data, definitions for various fields. Cleansed it to provide structure to the data. 
2. Perform statistical analysis of the data and find if any correlation can be obtained. 
3. To understand if ads click rate can be enhanced.  
4. I came up with two concepts through which their product can be improved with a focus on location of the user. 
5. The concepts are being developed with domain expertise from the company. 

## Individual Projects: 
In process of uploading more work. 

__Multi Class Classification: Tools used- MATLAB, Python, pyspark and some R for experimentation__

1. [Hand-Posture Recognition:](https://github.com/MrudulBanka1/Hand-Posture-Recognition)
- 5 class classification project to recognize different hand postures from motion captured Hand Postures. 
- Number of Instances: 78095, Number of Attributes: 38, Attribute Characteristics: Real

2. [Credit Card Fraud Detection:](https://github.com/MrudulBanka1/pyspark)

3. [Audio Classification:](https://github.com/MrudulBanka1/Audio-Classification/blob/master/urban_sound_classification.ipynb)
- Classification of audio Files which includes sounds of street music, drilling, gun shot, children playing, engine idling, car horn, siren, jackhammer and air conditioner. 

__Unsupervised learning: Tools used- Python, Jupyter Notebook__

1. [Weather prediction:](https://github.com/MrudulBanka1/Unsupervised_Learning)
- The minute weather dataset contains raw sensor measurements captured at one-minute intervals.
- This data comes from a weather station located in San Diego, California. Data was collected for a period of three years, from September 2011 to September 2014.

__Regression Analysis: Tools used - Sas, python, pyspark__

1. [Bitcoin-Regression:](https://github.com/MrudulBanka1/Bitcoin-Regression)
- This is a simple but in depth implementation of Multivariate Regression (MLR) analysis where the objective is to 
understand the statistical relationship between various parameters, look for best MLR model which gives us a 
satisfactory value of R squared to justify the model and by conducting hypothesis testing. 
- By using Regression analysis, we try to predict the price of bitcoin from a static dataset.

2. [California_Housing_median_house_value_prediction:](https://github.com/MrudulBanka1/pyspark)

__Deep Learning:__ 

1. [Cats vs Dogs:](https://github.com/MrudulBanka1/Portfolio/blob/master/Cats%20vs%20Dogs.ipynb)

__Natural Language processing: Tools used- Python, Pyspark, NLTK and other NLP libraries, Jupyter Notebook__

1. [The New York Times Topic Modeling:](https://github.com/MrudulBanka1/The-New-York-Times-Comments-Topic-Modeling)
- In this project we try to analyze the comments posted on articles of The New York Times between Jan - May for 2017 and 2018.
- We find 9 -15 different topics from the comments. Through the comments we try to predict the most popular topics and its keywords with their scores. 
2. [Sentiment Analysis for Twitter:](https://github.com/MrudulBanka1/Twitter_stream_Rest_api)
- We first scarp data from twitter by different means and predict the sentiment of the tweet by using a pre trained model.
- The graph shows how a sentiment changes for a particular topic over a number of tweets. 
3. [Twitter Racism Analyzer:](https://github.com/MrudulBanka1/Twitter_stream_Rest_api/blob/master/Twitter_racism_Analyzer.ipynb)
- Problem Statement: The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.
- Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.
- Evaluation Metric: The metric used for evaluating the performance of classification model would be F1-Score.

__Recommender Systems__
1. [Movielens:](https://github.com/MrudulBanka1/Recommender-Systems/blob/master/Movielens_content_collaborative_SVD.ipynb)

2. [Product Recommendation:](https://github.com/MrudulBanka1/Recommender-Systems/blob/master/Purchase_data_recommendation_engine.ipynb)


__Design of Experiment: Tools used - Sas__

1. [Coffee:](https://github.com/MrudulBanka1/Coffee)
- The goal of this study is to detect whether the coffee brand or the type of coffee machine have any effect on 
the acidity levels of the coffee. This is to allow coffee drinkers who are sensitive to acidic foods and drinks 
to be aware of what coffee brands or the brewing process that may be harmful. 
- We design an experiment to measure acidic levels (in pH) of three different types of coffee roast and two different 
types of coffee making techniques and analyze experimental data.

__Time Series Models__

1. [JetRail:](https://github.com/MrudulBanka1/Time_Series_Models)
- Problem Statement: You need to help Unicorn ventures with the decision. They usually invest in B2C start-ups less than 4 years old looking for pre-series A funding. In order to help Unicorn Ventures in their decision, you need to forecast the traffic on JetRail for the next 7 months. You are provided with traffic data of JetRail since inception in the test file.
- Evaluation Metrics: Root Mean Squre Error (RMSE) is the evaluation metric for this contest

__Simulation and Optimization__

__Fraud Detection: Python, pyspark__

1. [Credit Card Fraud Detection:](https://github.com/MrudulBanka1/pyspark)

__Image Processing and Video Analytics__

__Marketing Research__

__Sports Analytics__

__Classifiers and feature engineering functions__

1. [My_Classifiers:](https://github.com/MrudulBanka1/My_Classifiers)        
- This section contains the functions of all the classifiers which I have tried to build from scratch. 

 

## Resume

   __Click to download__
   
<a href="Resume_Mrudul_Banka.pdf">Mrudul Banka Resume </a>

## Contact Details

- [mrudulbanka@gmail.com](mailto:mrudulbanka@gmail.com?subject=[GitHub]%20Source%20Han%20Sans) 
- [mrudulpopatlal.banka@mavs.uta.edu](mailto:mrudulpopatlal.banka@mavs.uta.edu?subject=[GitHub]%20Source%20Han%20Sans)
- Cell phone: +1-682-551-9129

## To Know more about me
- [LinkedIn](https://www.linkedin.com/in/mrudulbanka/)
- [Kaggle](https://www.kaggle.com/mrudul)
   
   Ranking:  
   - Digit Recognizer - Top 3%
   - House price prediction - Top 10%
   - Jigsaw Unintended Bias in toxicity classification - Top 53%
   
- [Twitter](https://twitter.com/MRYDUL)
- [Facebook](https://www.facebook.com/mrudul.banka)

Thank you for visiting :smile: :+1: 
